query = "í…ìŠ¤íŠ¸ ìƒì„±"
print(f"ì¿¼ë¦¬: {query}\n")

for i, text in enumerate(texts):
    if any(term in text for term in ["í…ìŠ¤íŠ¸"]):  # í‚¤ì›Œë“œ ë§¤ì¹­ ì˜ˆì‹œ
        print(f"[{i}] {text[:100]}...")

# ì‚¬ìš©ì ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
query_texts = [
    "ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± ë°©ë²•",      # ì´ë¯¸ì§€ ìƒì„±
    "í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ì–¸ì–´ ëª¨ë¸ì„ ìƒì„±í•˜ê¸° ìœ„í•œ ë°©ë²•"              # í…ìŠ¤íŠ¸ ìƒì„±
]

# ê° ì¿¼ë¦¬ì— ëŒ€ì‘ë˜ëŠ” ì •ë‹µ ë¬¸ì„œ ì¸ë±ìŠ¤ (ì‚¬ëŒì´ ì§ì ‘ ì„ ì •)
# ì˜ˆ: df['abstract']ì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ indexë“¤
ground_truth = {
    0: [27, 76, 744, 880, 871, 1076, 1890, 1692, 383, 991, 1994 ],     # "ì´ë¯¸ì§€ ìƒì„±" ì¿¼ë¦¬ì— ëŒ€í•œ ì •ë‹µ ì¸ë±ìŠ¤ë“¤
    1: [489, 1621, 1972, 744,763, 888, 1074,1751, 1798]         # "í…ìŠ¤íŠ¸ ìƒì„±" ì¿¼ë¦¬ì— ëŒ€í•œ ì •ë‹µ ì¸ë±ìŠ¤ë“¤
}

# KorPatElectra ì„ë² ë”©
query_emb_kpe = model.encode(query_texts, device=device)

# LSA ì„ë² ë”©
query_tfidf = tfidf.transform(query_texts)
query_emb_lsa_raw = svd.transform(query_tfidf)
query_emb_lsa = Normalizer(copy=False).fit_transform(query_emb_lsa_raw)

# ê²€ìƒ‰ ë° í‰ê°€ í•¨ìˆ˜
def evaluate_queries(query_emb, doc_emb, ground_truth, model_name="Model", k=10):
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

    sims = cosine_similarity(query_emb, doc_emb)
    scores, recalls, mrrs = [], [], []

    for i, sim_row in enumerate(sims):
        true_set = set(ground_truth.get(i, []))
        if not true_set:
            continue

        topk_idx = np.argsort(sim_row)[::-1][:k]
        hits = [1 if idx in true_set else 0 for idx in topk_idx]

        precision = sum(hits) / k
        recall = sum(hits) / len(true_set)
        mrr = 0.0
        for rank, h in enumerate(hits, start=1):
            if h:
                mrr = 1.0 / rank
                break

        scores.append(precision)
        recalls.append(recall)
        mrrs.append(mrr)

    print(f"\nğŸ“Œ [{model_name}] Query-based Evaluation (Top-{k})")
    print(f"Precision@{k}: {np.mean(scores):.4f}")
    print(f"Recall@{k}:    {np.mean(recalls):.4f}")
    print(f"MRR@{k}:       {np.mean(mrrs):.4f}")

evaluate_queries(query_emb_kpe, X_kpe, ground_truth, model_name="KorPatElectra")
evaluate_queries(query_emb_lsa, X_lsa, ground_truth, model_name="LSA")
